import os
import json
import time
import numpy as np
import ot
import faiss
import csv

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ =================

# 1. æ–‡ä»¶å¤¹è·¯å¾„é…ç½®
SOURCE_DIR = "source_points"
TARGET_DIR = "target_points"

# 2. ä¿æŠ¤æœºåˆ¶ & å½’æ¡£
# 4090 æ˜¾å­˜å¾ˆå¤§ï¼Œè®¾ç½®å•è¾¹æœ€å¤§ç‚¹æ•°é™åˆ¶ (ä¾‹å¦‚ 100ä¸‡)
MAX_POINTS_LIMIT = 1000000
ARCHIVE_FILE = "batch_results_50_files.csv"

# 3. OT å‚æ•° (å®Œå…¨æ˜ å°„é…ç½®)
# è®¾ä¸º 1.0 ä»£è¡¨å°è¯•å°† Source çš„æ‰€æœ‰ç‚¹éƒ½åŒ¹é…åˆ° Target ä¸­
MASS_TO_TRANSPORT = 1.0
REG_STRENGTH = 0.05
CANDIDATE_K = 10

# ===========================================

# å°è¯•å¯¼å…¥ cupy ä»¥åˆ©ç”¨ 4090 åŠ é€Ÿ
try:
    import cupy as cp

    HAS_GPU = True
    print(f"âœ… æ£€æµ‹åˆ° GPU ç¯å¢ƒ (Cupy), å°†ä½¿ç”¨ RTX 4090 åŠ é€Ÿè®¡ç®—ã€‚")
except ImportError:
    HAS_GPU = False
    print("âš ï¸ æœªæ£€æµ‹åˆ° cupyï¼Œå°†ä½¿ç”¨ CPU è¿è¡Œã€‚")


def load_json_points(file_path):
    if not os.path.exists(file_path):
        return np.array([])

    with open(file_path, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            print(f"âš ï¸  JSON è§£æé”™è¯¯: {os.path.basename(file_path)}")
            return np.array([])

    # ================= å…¼å®¹æ€§å¢å¼º =================
    # å¦‚æœè¯»å–åˆ°çš„æ˜¯å­—å…¸ï¼Œå°è¯•è‡ªåŠ¨æå–å†…éƒ¨çš„åˆ—è¡¨æ•°æ®
    if isinstance(data, dict):
        # 1. ä¼˜å…ˆæŸ¥æ‰¾å¯èƒ½çš„é”®å
        possible_keys = ['target_points', 'source_points', 'points', 'data', 'target', 'coordinates']
        extracted = False
        for key in possible_keys:
            if key in data and isinstance(data[key], list):
                data = data[key]
                extracted = True
                break

        # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªæ˜¯ list ç±»å‹çš„ Value
        if not extracted:
            for v in data.values():
                if isinstance(v, list) and len(v) > 0:
                    data = v
                    extracted = True
                    break
    # ============================================

    # å†æ¬¡æ£€æŸ¥æ˜¯å¦ä¸ºåˆ—è¡¨ï¼Œå¦‚æœè¿˜ä¸æ˜¯ï¼Œè¯´æ˜æ ¼å¼çœŸçš„ä¸å¯¹
    if not isinstance(data, list):
        # print(f"âš ï¸  è­¦å‘Š: {os.path.basename(file_path)} ç»“æ„ä¸åŒ…å«åˆ—è¡¨æ•°æ®")
        return np.array([])

    points = []
    for item in data:
        # æƒ…å†µ A: åˆ—è¡¨ä¸­çš„é¡¹æ˜¯å­—å…¸ (e.g. {"x":1, "y":2})
        if isinstance(item, dict):
            x = item.get('x', item.get('lon', item.get('Longitude', 0)))
            y = item.get('y', item.get('lat', item.get('Latitude', 0)))
            points.append([float(x), float(y)])
        # æƒ…å†µ B: åˆ—è¡¨ä¸­çš„é¡¹æ˜¯åˆ—è¡¨/å…ƒç»„ (e.g. [1, 2])
        elif isinstance(item, (list, tuple)) and len(item) >= 2:
            x, y = item[0], item[1]
            points.append([float(x), float(y)])

    return np.array(points, dtype=np.float32)


def archive_result(data_dict):
    """å°†è¿è¡Œç»“æœè¿½åŠ å†™å…¥ CSV"""
    file_exists = os.path.isfile(ARCHIVE_FILE)
    with open(ARCHIVE_FILE, 'a', newline='') as f:
        fieldnames = ["timestamp", "dataset", "source_file", "n_source", "n_target",
                      "status", "device", "time_sec", "cost", "coverage", "m_value"]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        row = {k: data_dict.get(k, "N/A") for k in fieldnames}
        writer.writerow(row)


def run_task_standalone(dataset_name, filename):
    src_path = os.path.join(SOURCE_DIR, filename)
    tgt_path = os.path.join(TARGET_DIR, filename)

    print(f"\n{'=' * 60}")
    print(f"ğŸš€ ä»»åŠ¡å¯åŠ¨: [{filename}]")

    record = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "dataset": dataset_name,
        "source_file": filename,
        "device": "GPU" if HAS_GPU else "CPU",
        "m_value": MASS_TO_TRANSPORT
    }

    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(tgt_path):
        print(f"âŒ è·³è¿‡: Target ç›®å½•ä¸­æœªæ‰¾åˆ°å¯¹åº”æ–‡ä»¶ {filename}")
        record["status"] = "Skipped: Target Missing"
        archive_result(record)
        return

    # 2. åŠ è½½æ•°æ®
    A_raw = load_json_points(src_path)
    B_raw = load_json_points(tgt_path)
    n_a, n_b = len(A_raw), len(B_raw)

    record["n_source"] = n_a
    record["n_target"] = n_b
    print(f"   æ•°æ®é‡ -> Source: {n_a}, Target: {n_b}")

    # 3. ä¿æŠ¤æœºåˆ¶
    if n_a == 0 or n_b == 0:
        record["status"] = "Skipped: Empty File"
        archive_result(record);
        return

    if n_a > MAX_POINTS_LIMIT or n_b > MAX_POINTS_LIMIT:
        record["status"] = "Skipped: Too Large"
        archive_result(record);
        return

    try:
        t_start = time.time()

        # 4. é¢„å¤„ç† (Z-score + Faiss)
        print(f"   [Step 1] å»ºç«‹ç´¢å¼•å¹¶ç­›é€‰é‚»åŸŸ (K={CANDIDATE_K})...")
        A_norm = (A_raw - np.mean(A_raw, 0)) / (np.std(A_raw, 0) + 1e-8)
        B_norm = (B_raw - np.mean(B_raw, 0)) / (np.std(B_raw, 0) + 1e-8)

        d = A_norm.shape[1]
        index = faiss.IndexFlatL2(d)
        index.add(B_norm)

        _, I = index.search(A_norm, CANDIDATE_K)
        candidate_indices = np.unique(I)
        B_subset_norm = B_norm[candidate_indices]
        n_cand = len(candidate_indices)

        # 5. å‡†å¤‡ OT æ•°æ® (GPU/CPU)
        if HAS_GPU:
            xp = cp
            A_gpu = xp.asarray(A_norm)
            B_sub_gpu = xp.asarray(B_subset_norm)
            M = xp.array(ot.dist(A_gpu, B_sub_gpu))
            a = xp.ones(n_a)
            b = xp.ones(n_cand)
        else:
            xp = np
            M = ot.dist(A_norm, B_subset_norm)
            a = np.ones(n_a)
            b = np.ones(n_cand)

        M /= (M.max() + 1e-8)

        # 6. è®¡ç®— m (å®Œå…¨æ˜ å°„é€»è¾‘)
        m_calc = int(n_a * MASS_TO_TRANSPORT)
        m_final = min(m_calc, n_cand)

        print(f"   [Step 2] è®¡ç®— Partial OT (m={m_final}/{n_a})...")
        P = ot.partial.entropic_partial_wasserstein(
            a, b, M, m=m_final, reg=REG_STRENGTH, numItermax=500
        )

        # 7. è§£æç»“æœ
        if HAS_GPU: P = cp.asnumpy(P)

        max_probs = np.max(P, axis=1)
        target_indices_local = np.argmax(P, axis=1)
        matched_mask = max_probs > 1e-9

        rows = np.where(matched_mask)[0]
        cols_local = target_indices_local[matched_mask]
        cols_global = candidate_indices[cols_local]

        # 8. è®¡ç®—ç‰©ç† Cost
        final_cost = 0.0
        if len(rows) > 0:
            diff = A_raw[rows] - B_raw[cols_global]
            final_cost = np.sum(np.linalg.norm(diff, axis=1))

        elapsed = time.time() - t_start
        coverage = len(rows) / n_a

        record["status"] = "Success"
        record["time_sec"] = round(elapsed, 4)
        record["cost"] = round(final_cost, 4)
        record["coverage"] = round(coverage, 4)

        print(f"âœ… å®Œæˆ! è€—æ—¶: {elapsed:.2f}s | è¦†ç›–ç‡: {coverage * 100:.1f}% | Cost: {final_cost:.2f}")
        archive_result(record)

    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        record["status"] = f"Error: {str(e)}"
        archive_result(record)


if __name__ == "__main__":
    # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(SOURCE_DIR) or not os.path.exists(TARGET_DIR):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶å¤¹ '{SOURCE_DIR}' æˆ– '{TARGET_DIR}'")
        exit()

    # 1. æ‰«ææ‰€æœ‰ .json æ–‡ä»¶
    all_files = [f for f in os.listdir(SOURCE_DIR) if f.endswith(".json")]
    all_files.sort()  # æ’åºï¼Œä¿è¯é¡ºåºä¸€è‡´

    total_files = len(all_files)
    print(f"ğŸ“‚ æ‰«æå®Œæ¯•: åœ¨ '{SOURCE_DIR}' ä¸­å‘ç° {total_files} ä¸ªä»»åŠ¡æ–‡ä»¶")

    # 2. å¼€å§‹éå†å¾ªç¯
    for i, filename in enumerate(all_files):
        print(f"\nProcessing {i + 1}/{total_files}...")
        dataset_name = os.path.splitext(filename)[0]
        run_task_standalone(dataset_name, filename)

    print(f"\nğŸ‰ æ‰€æœ‰ {total_files} ä¸ªæ–‡ä»¶çš„ä»»åŠ¡å·²å…¨éƒ¨ç»“æŸã€‚")
    print(f"ğŸ“„ ç»“æœå·²ä¿å­˜åœ¨: {ARCHIVE_FILE}")

    if __name__ == "__main__":
        # 1. è·¯å¾„æ£€æŸ¥ (ä¿æŒä¸ batch_experiment_pics ä¸€è‡´çš„æ–‡ä»¶å¤¹å)
        if not os.path.exists(SOURCE_DIR) or not os.path.exists(TARGET_DIR):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶å¤¹è·¯å¾„ '{SOURCE_DIR}' æˆ– '{TARGET_DIR}'")
            exit(1)

        # 2. è·å–æ–‡ä»¶åˆ—è¡¨ (å®Œå…¨å¤åˆ¶ batch_experiment_pics.py çš„é€»è¾‘)
        # ä½¿ç”¨é›†åˆ (set) è·å–æ–‡ä»¶åï¼Œä»¥ä¾¿å–äº¤é›†
        source_files = {f for f in os.listdir(SOURCE_DIR) if f.endswith('.json')}
        target_files = {f for f in os.listdir(TARGET_DIR) if f.endswith('.json')}

        # å–äº¤é›†å¹¶æ’åºï¼Œç¡®ä¿åªå¤„ç†ä¸¤è¾¹éƒ½æœ‰çš„æ–‡ä»¶ï¼Œä¸”é¡ºåºä¸€è‡´
        all_files = sorted(source_files & target_files)

        print(f"=== Debug æ¨¡å¼: å‡†å¤‡å¤„ç† {len(all_files)} ç»„æ•°æ® ===")
        print(f"ğŸ“‚ Source ç›®å½•: {SOURCE_DIR} (å…± {len(source_files)} ä¸ª json)")
        print(f"ğŸ“‚ Target ç›®å½•: {TARGET_DIR} (å…± {len(target_files)} ä¸ª json)")
        print(f"ğŸ”— åŒ¹é…æˆåŠŸ (äº¤é›†): {len(all_files)} ä¸ªæ–‡ä»¶")
        print("-" * 60)

        # 3. éå†æ‰§è¡Œ
        for idx, filename in enumerate(all_files):
            print(f"\n[{idx + 1}/{len(all_files)}] å¤„ç†: {filename}")
            dataset_name = os.path.splitext(filename)[0]
            run_task_standalone(dataset_name, filename)

        print("\n" + "=" * 60)
        print(f"ğŸ‰ å…¨éƒ¨å®Œæˆ! ç»“æœå·²ä¿å­˜è‡³: {ARCHIVE_FILE}")